# Focused Web Crawler - Tomas Pasiecznik

# Imports
from bs4 import BeautifulSoup
from urllib.request import urlopen
import queue
import re
import os

# Example Focus Topic Chosen: Operating Systems
seed_urls = ['https://en.wikipedia.org/wiki/Operating_system', 'https://en.wikipedia.org/wiki/Microsoft_Windows']
related_terms = ['operating system', 'software', 'computer', 'Windows 10', 'Mac', 'Linux', 'Unix', 'Android', 'iOS',
                 'Program']

# Add output location for files generated by the Web Crawler.
crawler_output_location = ''

# Number of unique pages to collect.
limit = 500


# Focused Crawler
def crawler(seeds, terms):
    # Initializing Variables
    q = queue.Queue()
    visited_urls = []
    saved_urls = []
    saved_titles = []
    page_counter = 0
    term_counter = 0

    # Putting Seed URLs into Queue
    for url in seeds:
        q.put(url)

    # Get Page Content Function
    def get_page_content(_url):
        soup = BeautifulSoup(urlopen(_url).read().decode('utf-8'), 'html.parser')
        return soup

    # Title Cleaner Function
    def clean_title(_title):
        invalid = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
        cleaned_title = ''

        for i in invalid:
            cleaned_title = _title.replace(i, '')

        return cleaned_title

    # Save Output Functions
    def save_crawled_page(title_text, html, _url, path):
        if not os.path.exists(path + 'crawler_output'):
            os.makedirs(path + 'crawler_output')
        saved_urls.append(_url)
        saved_titles.append(title_text)

        # The following two lines prevent errors when some special characters are present in the Wikipedia titles.
        filename = re.sub('\\\\', '_', title_text)
        filename = re.sub('/', '_', filename)
        filename = re.sub('–', '_', filename)

        path = os.path.join(path + 'crawler_output', str(page_counter) + ". " + filename + '.html')
        file = open(path, 'w', encoding='utf-8', errors='ignore')
        file.write(str(html))
        file.close()
        print('Page Saved | ' + str(page_counter) + ' | Title: ' + title_text)

    def save_all_urls(_url, path):
        print("Saving all URLs...")
        if not os.path.exists(path + 'crawler_output'):
            os.makedirs(path + 'crawler_output')
        file = open(path + 'crawler_output/crawled_urls.txt', 'w')
        count = 1
        for url_to_save in saved_urls:
            file.write(str(count) + ': ' + url_to_save + '\n')
            count += 1
        file.close()
        print("All URLs saved successfully.")

    # Extraction of Outgoing URLs Function
    def get_urls(content):
        new_urls = []
        links_found = content.find_all('a')
        for link in links_found:
            new_urls.append(link.get('href'))
        return new_urls

    # Domain Checker Function
    def domain_check(outgoing_url):
        if outgoing_url is None:
            return False
        if re.search('#', outgoing_url):
            return False

        if outgoing_url.count('/') > 2:
            return False
        if outgoing_url.count('\\') > 0:
            return False
        if outgoing_url.count(':') > 0:
            return False
        if outgoing_url.count('.') > 0:
            return False

        if re.match('^/wiki/', outgoing_url):
            return True
        else:
            return False

    # Queue Traversal
    print('――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――')
    print('| Crawler Activated |')
    while q.qsize() != 0:
        print('――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――')
        url = q.get()
        page_content = get_page_content(url)
        if page_content is None:
            continue
        main_text = page_content.get_text()
        title = page_content.title.string
        title = clean_title(title)

        # Related Terms Checker
        print('Searching for related terms in page: ' + url)
        for term in terms:
            if re.search(term, main_text, re.I):
                term_counter += 1
                if term_counter >= 2 and title not in saved_titles:
                    page_counter += 1
                    save_crawled_page(title, page_content, url, crawler_output_location)
                    break
        term_counter = 0

        # Page Limit Checker
        if page_counter >= limit:
            break

        # Extraction of Outgoing URLs - Not run when page limit is hit.
        print('Searching for outgoing URLs in page...')
        outgoing = get_urls(page_content)
        outgoing_cleaned = []

        # Reformatting Outgoing URLs
        for domain in outgoing:
            if domain_check(domain):
                domain = "https://en.wikipedia.org" + domain
                outgoing_cleaned.append(domain)

        for _domain in outgoing_cleaned:
            if _domain not in visited_urls:
                q.put(_domain)
                visited_urls.append(_domain)

    # Save Crawled URLs
    print('――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――')
    save_all_urls(saved_urls, crawler_output_location)
    print('――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――')
    print('| Crawler Deactivated |')
    print('――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――')


# Calling The Crawler
crawler(seed_urls, related_terms)
